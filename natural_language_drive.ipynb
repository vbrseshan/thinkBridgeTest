{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "820181e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files found.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 131\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMost relevant document: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocument_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 131\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 113\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    111\u001b[0m folder_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1X1Wd1D2mSnCP-llYQghglTrMDcnAIh1a\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    112\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 113\u001b[0m \u001b[43mdrive_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m vectorizer, tfidf_matrix \u001b[38;5;241m=\u001b[39m drive_search\u001b[38;5;241m.\u001b[39mload_index()\n\u001b[0;32m    115\u001b[0m queries \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow To Create an Amazon AWS Free Tier Account?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat are components of s3?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is S3 replication?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    119\u001b[0m ]\n",
      "Cell \u001b[1;32mIn[7], line 95\u001b[0m, in \u001b[0;36mGoogleDriveSearch.build_index\u001b[1;34m(self, folder_id, output_path)\u001b[0m\n\u001b[0;32m     92\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_documents(output_path)\n\u001b[0;32m     94\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m---> 95\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     98\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump((vectorizer, tfidf_matrix), f)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2131\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2126\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2127\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2128\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2129\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2130\u001b[0m )\n\u001b[1;32m-> 2131\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2133\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1380\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m             )\n\u001b[0;32m   1385\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1390\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1293\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1295\u001b[0m         )\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from flask import Flask, request\n",
    "from flask_sqlalchemy import SQLAlchemy\n",
    "\n",
    "\n",
    "\n",
    "cresentials = 'drive_creds.json'\n",
    "API_NAME = 'drive'\n",
    "API_VERSION = 'v3'\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.readonly']\n",
    "app = Flask(__name__)\n",
    "app.config['SQLALCHEMY_DATABASE_URI'] = 'your_database_uri'\n",
    "db = SQLAlchemy(app)\n",
    "\n",
    "\n",
    "class Document(db.Model):\n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    name = db.Column(db.String(100), nullable=False)\n",
    "    content = db.Column(db.Text, nullable=False)\n",
    "\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "class GoogleDriveSearch:\n",
    "    def __init__(self):\n",
    "        self.service = self._authenticate()\n",
    "\n",
    "    def _authenticate(self):\n",
    "        from google.oauth2 import service_account\n",
    "\n",
    "        creds = service_account.Credentials.from_service_account_file(cresentials, scopes=SCOPES)\n",
    "        return build(API_NAME, API_VERSION, credentials=creds)\n",
    "\n",
    "    def _download_document(self, file_id):\n",
    "        request = self.service.files().get_media(fileId=file_id)\n",
    "        fh = io.BytesIO()\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            status, done = downloader.next_chunk()\n",
    "\n",
    "        fh.seek(0)\n",
    "        return fh\n",
    "\n",
    "    def download_documents(self, folder_id, output_path):\n",
    "        results = self.service.files().list(q=f\"'{folder_id}' in parents and mimeType!='application/vnd.google-apps.folder'\",\n",
    "                                            fields=\"files(id, name)\").execute()\n",
    "        files = results.get('files', [])\n",
    "\n",
    "        if not files:\n",
    "            print('No files found.')\n",
    "            return\n",
    "\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        for file in files:\n",
    "            print(f\"Downloading {file['name']}...\")\n",
    "            file_id = file['id']\n",
    "            fh = self._download_document(file_id)\n",
    "\n",
    "            with open(os.path.join(output_path, file['name']), 'wb') as f:\n",
    "                f.write(fh.read())\n",
    "\n",
    "            print(f\"Downloaded {file['name']} to {output_path}.\")\n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        sentences = sent_tokenize(text)\n",
    "        preprocessed_sentences = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            words = word_tokenize(sentence.lower())\n",
    "            words = [word for word in words if word.isalpha() and word not in STOPWORDS]\n",
    "            preprocessed_sentences.append(' '.join(words))\n",
    "\n",
    "        return ' '.join(preprocessed_sentences)\n",
    "\n",
    "    def _load_documents(self, folder_path):\n",
    "        documents = []\n",
    "\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "\n",
    "            if os.path.isfile(file_path):\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                    preprocessed_text = self._preprocess_text(text)\n",
    "                    documents.append(preprocessed_text)\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def build_index(self, folder_id, output_path):\n",
    "        self.download_documents(folder_id, output_path)\n",
    "        documents = self._load_documents(output_path)\n",
    "\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "        with open('tfidf.pickle', 'wb') as f:\n",
    "            pickle.dump((vectorizer, tfidf_matrix), f)\n",
    "\n",
    "        print(\"Index built successfully.\")\n",
    "\n",
    "    def load_index(self):\n",
    "        with open('tfidf.pickle', 'rb') as f:\n",
    "            vectorizer, tfidf_matrix = pickle.load(f)\n",
    "\n",
    "        return vectorizer, tfidf_matrix\n",
    "\n",
    "\n",
    "def main():\n",
    "    drive_search = GoogleDriveSearch()\n",
    "    folder_id = '1X1Wd1D2mSnCP-llYQghglTrMDcnAIh1a'\n",
    "    output_path = 'output'\n",
    "    drive_search.build_index(folder_id, output_path)\n",
    "    vectorizer, tfidf_matrix = drive_search.load_index()\n",
    "    queries = [\n",
    "        \"How To Create an Amazon AWS Free Tier Account?\",\n",
    "        \"what are components of s3?\",\n",
    "        \"what is S3 replication?\",\n",
    "    ]\n",
    "    for file in os.listdir(output_path):\n",
    "        file_path = os.path.join(output_path, file)\n",
    "\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "\n",
    "            document = Document(name=file, content=content)\n",
    "            db.session.add(document)\n",
    "\n",
    "        db.session.commit()\n",
    "    for query in queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        preprocessed_query = drive_search._preprocess_text(query)\n",
    "        query_vector = vectorizer.transform([preprocessed_query])\n",
    "        similarities = cosine_similarity(query_vector, tfidf_matrix)\n",
    "        most_similar_index = similarities.argmax()\n",
    "        document_name = os.listdir(output_path)[most_similar_index]\n",
    "        print(f\"Most relevant document: {document_name}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a757b92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\win 10\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\win 10\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\win 10\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\win 10\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\win 10\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\win 10\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2373e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\win\n",
      "[nltk_data]     10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75284a61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
